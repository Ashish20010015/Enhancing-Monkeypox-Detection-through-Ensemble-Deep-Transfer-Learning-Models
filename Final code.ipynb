{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install livelossplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries: The code starts by importing necessary libraries and modules including ones for data handling, model building, evaluation metrics, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.image as mimg\n",
    "%matplotlib inline\n",
    "from PIL import Image\n",
    "from scipy import misc\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, Concatenate, BatchNormalization\n",
    "from keras.layers import UpSampling2D, Dropout, Add, Multiply, Subtract, AveragePooling2D\n",
    "from keras.layers import Activation, SpatialDropout2D\n",
    "from keras.layers import Dense, Lambda\n",
    "from keras.layers import GlobalAveragePooling2D, Reshape, Dense, Permute, Flatten\n",
    "\n",
    "from keras.utils import plot_model\n",
    "\n",
    "from keras.optimizers import * \n",
    "from keras.callbacks import *\n",
    "from keras.activations import *\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from livelossplot import PlotLossesKeras\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "from tensorflow.keras.applications import DenseNet201, DenseNet121,DenseNet169,InceptionResNetV2,ResNet152V2\n",
    "from scipy.ndimage import median_filter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Seeds: Seeds for random number generation are set to ensure reproducibility across runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining Data Directories: Paths to directories containing training, validation, and test data are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loc = '/kaggle/input/mpox-skin-lesion-dataset-version-20-msld-v20/Augmented Images/Augmented Images/FOLDS_AUG/fold5_AUG/Train'\n",
    "val_loc = '/kaggle/input/mpox-skin-lesion-dataset-version-20-msld-v20/Original Images/Original Images/FOLDS/fold5/Valid'\n",
    "test_loc = '/kaggle/input/mpox-skin-lesion-dataset-version-20-msld-v20/Original Images/Original Images/FOLDS/fold5/Test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch size for training and validation data generators is defined. It's set to 32 ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use different batch size  for training and validation to make sure that the model is exposed to a wide variety of data during each training and validation. Then comapre both rseult.\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Generators: Image data generators are initialized for training, validation, and test data. These generators will preprocess the images and yield batches of data during model training.\n",
    "These lines of code are using the `ImageDataGenerator` class from Keras to create data generators for loading and augmenting images from directories. Here's a breakdown:\n",
    "\n",
    "1. **ImageDataGenerator**:\n",
    "   - `ImageDataGenerator` is a utility class in Keras that generates batches of tensor image data with real-time data augmentation.\n",
    "\n",
    "2. **Data Loading**:\n",
    "   - `train_data`, `val_data`, and `test_data` are data generators created for training, validation, and testing datasets respectively.\n",
    "   - Each data generator loads images from a specific directory (`train_loc`, `val_loc`, `test_loc`) and generates batches of augmented images during training.\n",
    "\n",
    "3. **Parameters**:\n",
    "   - `directory`: Specifies the path to the directory containing the images.\n",
    "   - `target_size`: Tuple specifying the height and width to which all images will be resized during loading.\n",
    "   - `batch_size`: The number of samples in each batch of data generated.\n",
    "   - `shuffle`: Boolean indicating whether to shuffle the data after each epoch.\n",
    "   - `seed`: Random seed for shuffling and transformations.\n",
    "\n",
    "4. **Flow from Directory**:\n",
    "   - `flow_from_directory` method is called on each `ImageDataGenerator` object to generate batches of data from the specified directories.\n",
    "   - It automatically infers the labels from the subdirectory structure and assigns them to the images.\n",
    "\n",
    "5. **Training, Validation, and Testing Data**:\n",
    "   - `train_data`: Data generator for training dataset.\n",
    "   - `val_data`: Data generator for validation dataset.\n",
    "   - `test_data`: Data generator for testing dataset.\n",
    "\n",
    "These data generators are suitable for training convolutional neural networks (CNNs) on image classification tasks where the dataset is organized into separate directories for each class. The generators load images in batches, which is memory efficient and allows for real-time data augmentation during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trdata = ImageDataGenerator()\n",
    "train_data = trdata.flow_from_directory(directory=train_loc, target_size=(224,224), batch_size=BATCH_SIZE, shuffle=True, seed=42)\n",
    "\n",
    "vdata = ImageDataGenerator()\n",
    "val_data = vdata.flow_from_directory(directory=val_loc, target_size=(224,224), batch_size=BATCH_SIZE, shuffle=True, seed=42)\n",
    "\n",
    "tsdata = ImageDataGenerator()\n",
    "test_data = tsdata.flow_from_directory(directory=test_loc, target_size=(224,224), batch_size=BATCH_SIZE, shuffle=False, seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Creation Function: The create_model function is defined to build a classification model using one of the specified pre-trained CNN architectures (DenseNet121,DenseNet169, ResNet152V2, InceptionResNetV2, DenseNet201). The function adds custom fully connected layers on top of the pre-trained base and compiles the model with specified optimizer and loss function.\n",
    "\n",
    "This `create_model` function constructs a convolutional neural network (CNN) based on the specified model architecture (`model_name`). Here's how it works:\n",
    "\n",
    "- **Inputs**:\n",
    "  - `model_name`: Name of the model architecture to be used (`DenseNet121`, `DenseNet169`, `InceptionResNetV2`, `ResNet152V2`, or `DenseNet201`).\n",
    "  - `input_shape`: Shape of the input data (e.g., `(height, width, channels)`).\n",
    "  - `n_classes`: Number of classes for classification.\n",
    "  - `optimizer`: Optimizer used for training the model.\n",
    "  - `fine_tune`: Boolean indicating whether fine-tuning is to be performed.\n",
    "\n",
    "- **Model Construction**:\n",
    "  - Based on the `model_name`, it initializes the corresponding pre-trained CNN model (e.g., DenseNet121, DenseNet169) with pre-trained weights from ImageNet.\n",
    "  - The top layers of the pre-trained model are removed, leaving only the convolutional base.\n",
    "  - Additional layers are added on top of the convolutional base for custom classification:\n",
    "    - `GlobalAveragePooling2D`: Performs global average pooling operation over the spatial dimensions of the input. This reduces each feature map to a single number by taking the average.\n",
    "    - `Flatten`: Flattens the input to a one-dimensional array.\n",
    "    - `Dense` layers with ReLU activation: These fully connected layers introduce non-linearity to the model.\n",
    "    - `Dropout`: Regularization technique to prevent overfitting by randomly dropping a fraction of units (0.2 and 0.3) during training.\n",
    "    - Final `Dense` layer with softmax activation: Produces output probabilities for each class.\n",
    "\n",
    "- **Model Compilation**:\n",
    "  - Compiles the model using the specified `optimizer` and loss function (`categorical_crossentropy` for multi-class classification).\n",
    "  - Metrics are set to `'accuracy'` for monitoring training and validation accuracy.\n",
    "\n",
    "- **Output**:\n",
    "  - Returns the compiled model.\n",
    "\n",
    "This function allows for creating various CNN architectures with customizable input shape, number of classes, and optimizer. It leverages transfer learning by initializing pre-trained models and fine-tuning them for the specific classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, input_shape, n_classes, optimizer, fine_tune):\n",
    "    if model_name == 'DenseNet121':\n",
    "        conv_base = DenseNet121(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif model_name == 'DenseNet169':\n",
    "        conv_base = DenseNet169(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif model_name == 'InceptionResNetV2':\n",
    "        conv_base = InceptionResNetV2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif model_name == 'ResNet152V2':\n",
    "        conv_base = ResNet152V2(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    elif model_name == 'DenseNet201':\n",
    "        conv_base = DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Invalid model name!\")\n",
    "    \n",
    "    top_model = conv_base.output\n",
    "    top_model = GlobalAveragePooling2D()(top_model)\n",
    "    top_model = Flatten(name=\"flatten\")(top_model)\n",
    "    top_model = Dense(128, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.2)(top_model)    \n",
    "    top_model = Dense(64, activation='relu')(top_model)\n",
    "    top_model = Dropout(0.3)(top_model)\n",
    "    output_layer = Dense(n_classes, activation='softmax')(top_model)\n",
    "    \n",
    "    model = Model(inputs=conv_base.input, outputs=output_layer)\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In situations where the hard voting predicts one class as the majority and the soft voting predicts a different class, but the actual class is neither of them, it indicates a disagreement among the ensemble models. This scenario can occur due to various reasons such as noise in the data, mislabeling, or inherent uncertainty in the classification task.\n",
    "To handle such situations and improve the robustness of your classifier, you can consider the following steps:\n",
    "\n",
    "1.\tThreshold Adjustment: Adjust the threshold for considering the majority in hard voting. Instead of a simple majority vote, you can require a higher threshold (e.g., more than half of the models) to agree on a class prediction before making a decision based on hard voting.\n",
    "\n",
    "2.\tConfidence Score: Calculate a confidence score for each prediction based on the agreement among the models. If there is high agreement among the models, you can trust the prediction more. Conversely, if there is disagreement, you can assign a lower confidence score to the prediction.\n",
    "\n",
    "3.\tEnsemble Weights: Assign different weights to the predictions of individual models based on their performance or reliability. Models with higher performance or higher confidence scores can be given more weight in the final decision.\n",
    "\n",
    "4.\tPost-Processing Techniques: Apply post-processing techniques such as smoothing or filtering to the predictions to remove noise or outliers.\n",
    "\n",
    "5.\tHuman-in-the-Loop: Incorporate human feedback or domain knowledge to resolve conflicting predictions. Human experts can provide valuable insights in ambiguous cases.\n",
    "\n",
    "6.\tRevisiting Training Data: Reevaluate the training data and labels to ensure accuracy and consistency. Retraining the models with improved data quality may lead to better performance.\n",
    "\n",
    "7.\tModel Diversity: Ensure diversity among the ensemble models by using different architectures, hyperparameters, or training data. This can help reduce the likelihood of all models making the same errors.\n",
    "\n",
    "8.\tError Analysis: Conduct thorough error analysis to identify patterns in misclassifications and potential causes. This can guide future improvements in data collection, preprocessing, or model architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine hard and soft voting, you can first compute the hard voting predictions and then compute the soft voting predictions only for the cases where there is no clear majority in the hard voting predictions.\n",
    "By combining both hard and soft voting, you can leverage the benefits of both approaches and potentially improve the overall accuracy of your ensemble classifier.\n",
    "\n",
    "•\tI've added a function combined_voting to perform ensemble voting with confidence score and threshold adjustment.\n",
    "\n",
    "•\tInside train_evaluate_ensemble, after evaluating individual models and obtaining their predictions, I've calculated ensemble predictions using hard voting, soft voting, and the combined approach.\n",
    "\n",
    "•\tThe combined_voting function takes the predictions from individual models, calculates the confidence scores, and then combines the predictions based on the threshold.\n",
    "\n",
    "•\tFinally, I've evaluated the combined predictions and printed the classification report and confusion matrix for the combined model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_voting(test_predictions, hard_voting_predictions, soft_voting_predictions, threshold):\n",
    "    confidence_scores = np.mean(np.max(test_predictions, axis=1), axis=0)\n",
    "\n",
    "    combined_predictions = []\n",
    "\n",
    "    min_len = min(len(hard_voting_predictions), len(soft_voting_predictions))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if confidence_scores[i] >= threshold:\n",
    "            combined_predictions.append(hard_voting_predictions[i])\n",
    "        else:\n",
    "            combined_predictions.append(soft_voting_predictions[i])\n",
    "\n",
    "    combined_predictions = np.array(combined_predictions)\n",
    "    return combined_predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Post-processing techniques can be applied after obtaining the combined predictions to refine the final output. In the provided code, we can apply post-processing techniques such as filtering or smoothing to the predictions to remove noise or outliers and  to improve the quality of the predictions. \n",
    "\n",
    "Smoothing and filtering are both techniques used in signal and image processing to enhance or modify data. While they serve similar purposes, they are typically used in slightly different contexts and may employ different mathematical methods.\n",
    "\n",
    "1.\tSmoothing:\n",
    "\n",
    "•\tSmoothing is a process of reducing the noise or high-frequency variations in a signal or an image.\n",
    "\n",
    "•\tIt aims to create a smoother version of the data by averaging or interpolating neighboring values.\n",
    "\n",
    "•\tSmoothing is often used to remove noise, blur sharp edges, or simplify complex patterns.\n",
    "\n",
    "•\tCommon smoothing techniques include Gaussian smoothing, median filtering, and moving average filtering.\n",
    "\n",
    "•\tSmoothing is commonly applied to data that exhibit random fluctuations or high-frequency noise.\n",
    "\n",
    "2.\tFiltering:\n",
    "\n",
    "•\tFiltering refers to the process of modifying or extracting specific components from a signal or an image using a filter.\n",
    "\n",
    "•\tFilters can be designed to enhance certain features, suppress noise, or extract relevant information from the data.\n",
    "\n",
    "•\tFiltering can be categorized into various types such as low-pass, high-pass, band-pass, and notch filters, each designed to address specific frequency components.\n",
    "\n",
    "•\tFiltering can be linear or nonlinear, depending on the characteristics of the filter.\n",
    "\n",
    "•\tFiltering is a broader concept that encompasses various operations, including smoothing, sharpening, edge detection, and feature extraction.\n",
    "\n",
    "In summary, smoothing is a specific type of filtering operation that focuses on reducing noise and creating a smoother version of the data, while filtering encompasses a broader range of operations aimed at modifying or extracting specific components from the data. Smoothing is often used as a preprocessing step before further analysis, while filtering can serve multiple purposes depending on the specific application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tI've defined a function apply_post_processing to apply median filtering to the combined predictions.\n",
    "\n",
    "•\tThe apply_post_processing function takes the combined predictions and applies the median filter with a specified filter size (in this case, 3x3).\n",
    "\n",
    "•\tAfter obtaining the combined predictions, we apply the post-processing technique to filter the predictions.\n",
    "\n",
    "•\tFinally, we evaluate the filtered predictions and print the classification report and confusion matrix for the combined model after post-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply post-processing technique - Median Filtering\n",
    "def apply_post_processing(predictions, filter_size):\n",
    "    filtered_predictions = []\n",
    "    for prediction in predictions:\n",
    "        filtered_prediction = median_filter(prediction, size=filter_size)\n",
    "        filtered_predictions.append(filtered_prediction)\n",
    "    return np.array(filtered_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function `train_evaluate_ensemble` is designed to train, evaluate, and ensemble multiple models. Let's break down its functionality:\n",
    "\n",
    "1. **Inputs**:\n",
    "   - `models`: A list containing names of different models to be used.\n",
    "   - `input_shape`: The shape of the input data.\n",
    "   - `n_classes`: The number of classes in the classification task.\n",
    "   - `optimizer`: The optimizer used for training the models.\n",
    "   - `fine_tune`: A boolean indicating whether fine-tuning is to be performed.\n",
    "\n",
    "2. **Initialization**:\n",
    "   - `model_list`: An empty list to store the trained models.\n",
    "   - `acc_individual_models`: A dictionary to store the accuracy of individual models.\n",
    "\n",
    "3. **Training and Evaluation of Individual Models**:\n",
    "   - Iterate through each model in the `models` list.\n",
    "   - Create the model using `create_model` function.\n",
    "   - Train the model using training data (`train_data`) and validate on validation data (`val_data`).\n",
    "   - Evaluate the trained model on the test data (`test_data`).\n",
    "   - Store the accuracy of each individual model in `acc_individual_models` dictionary.\n",
    "   - Print accuracy, classification report, and confusion matrix for each individual model.\n",
    "\n",
    "4. **Ensemble Predictions**:\n",
    "   - Generate predictions for each individual model on the test data.\n",
    "   - Perform ensemble using majority voting and soft voting techniques.\n",
    "   - Calculate the accuracy of the ensemble models.\n",
    "\n",
    "5. **Post-processing and Evaluation of Combined Predictions**:\n",
    "   - Combine predictions from individual models.\n",
    "   - Apply post-processing technique (median filtering) using `apply_post_processing` function.\n",
    "   - Evaluate the accuracy of the combined predictions after post-processing.\n",
    "   - Print accuracy, classification report, and confusion matrix for the combined model.\n",
    "\n",
    "6. **Save Combined Filtered Model**:\n",
    "   - Save the combined filtered model to a specified path.\n",
    "\n",
    "7. **Accuracy Comparison Visualization**:\n",
    "   - Plot a bar graph showing the accuracy comparison between individual models, ensemble models, and the combined filtered model.\n",
    "\n",
    "8. **ROC Analysis for Ensemble Model**:\n",
    "   - Perform ROC analysis for the ensemble model with combined filtered predictions.\n",
    "   - Plot ROC curves for each class.\n",
    "\n",
    "9. **Outputs**:\n",
    "   - Return the accuracy of the combined filtered model, the path where the model is saved, and the list of trained models.\n",
    "\n",
    "This function provides a comprehensive analysis of the performance of individual models, ensemble models, and the combined filtered model, along with visualizations to aid in understanding the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train, evaluate and ensemble models\n",
    "def train_evaluate_ensemble(models, input_shape, n_classes, optimizer, fine_tune):\n",
    "    model_list = []\n",
    "    acc_individual_models = {}\n",
    "    \n",
    "    for model_name in models:\n",
    "        model = create_model(model_name, input_shape, n_classes, optimizer, fine_tune)\n",
    "        history = model.fit(train_data, epochs=100, steps_per_epoch=train_data.n//train_data.batch_size,\n",
    "                            class_weight=class_weights, validation_data=val_data, validation_steps=val_data.n//val_data.batch_size,\n",
    "                            callbacks=[checkpoint, early_stop, PlotLossesKeras()], verbose=0)\n",
    "        model_list.append(model)\n",
    "        \n",
    "        # Evaluate individual models\n",
    "        model_preds = model.predict(test_data, test_data.samples//test_data.batch_size+1)\n",
    "        model_pred_classes = np.argmax(model_preds , axis=1)\n",
    "        true_classes = test_data.classes\n",
    "        acc_individual_models[model_name] = accuracy_score(true_classes, model_pred_classes)\n",
    "        \n",
    "        print(f\"Accuracy of {model_name}: {acc_individual_models[model_name] * 100:.2f}%\")\n",
    "        print('Classification Report:')\n",
    "        print(classification_report(true_classes, model_pred_classes))\n",
    "        print('Confusion Matrix:')\n",
    "        print(confusion_matrix(true_classes, model_pred_classes))\n",
    "    \n",
    "    # Predictions\n",
    "    test_predictions = []\n",
    "    for model in model_list:\n",
    "        test_predictions.append(model.predict(test_data, test_data.samples//test_data.batch_size+1))\n",
    "    \n",
    "    # Ensemble using majority voting\n",
    "    majority_voting_predictions = np.argmax(np.sum(test_predictions, axis=0), axis=1)\n",
    "    \n",
    "    # Ensemble using soft voting\n",
    "    soft_voting_predictions = np.argmax(np.mean(test_predictions, axis=0), axis=1)\n",
    "    \n",
    "    # Evaluate ensemble models\n",
    "    acc_majority_voting = accuracy_score(true_classes, majority_voting_predictions)\n",
    "    acc_soft_voting = accuracy_score(true_classes, soft_voting_predictions)\n",
    "    \n",
    "    \n",
    "    print(f\"Accuracy of Ensemble Model (Majority Voting): {acc_majority_voting * 100:.2f}%\")\n",
    "    print(f\"Accuracy of Ensemble Model (Soft Voting): {acc_soft_voting * 100:.2f}%\")\n",
    "    \n",
    "    print('Classification Report for Ensemble Model (Majority Voting):')\n",
    "    print(classification_report(true_classes, majority_voting_predictions))\n",
    "    print('Confusion Matrix for Ensemble Model (Majority Voting):')\n",
    "    print(confusion_matrix(true_classes, majority_voting_predictions))\n",
    "    \n",
    "    print('Classification Report for Ensemble Model (Soft Voting):')\n",
    "    print(classification_report(true_classes, soft_voting_predictions))\n",
    "    print('Confusion Matrix for Ensemble Model (Soft Voting):')\n",
    "    print(confusion_matrix(true_classes, soft_voting_predictions))\n",
    "\n",
    "    # Combine predictions\n",
    "    combined_predictions = combined_voting(test_predictions, majority_voting_predictions, soft_voting_predictions, threshold)\n",
    "    \n",
    "    # Apply post-processing technique\n",
    "    filtered_predictions = apply_post_processing(combined_predictions, filter_size=3)\n",
    "    \n",
    "    # Evaluate combined predictions after post-processing\n",
    "    acc_combined_filtered = accuracy_score(true_classes, filtered_predictions)\n",
    "\n",
    "    print(f\"Accuracy of Combined Model after Post-Processing: {acc_combined_filtered * 100:.2f}%\")\n",
    "    print('Classification Report for Combined Model after Post-Processing:')\n",
    "    print(classification_report(true_classes, filtered_predictions))\n",
    "    print('Confusion Matrix for Combined Model after Post-Processing:')\n",
    "    print(confusion_matrix(true_classes, filtered_predictions))\n",
    "\n",
    "    # Save combined filtered model\n",
    "    combined_filtered_model_path = \"../working/combined_filtered_model.h5\"\n",
    "    combined_filtered_model = create_model(models[0], input_shape, n_classes, optimizer, fine_tune)  # Assuming the first model in the list is used for creating the combined model\n",
    "    combined_filtered_model.save(combined_filtered_model_path)\n",
    "    print(f\"Combined Filtered Model saved at: {combined_filtered_model_path}\")\n",
    "\n",
    "    # Plotting accuracy comparison graph\n",
    "    models.append('Ensemble (Majority Voting)')\n",
    "    models.append('Ensemble (Soft Voting)')\n",
    "    models.append('Ensemble (combined_filtered)')\n",
    "    accuracies = [acc_individual_models[model_name] for model_name in models[:-3]]\n",
    "    accuracies.extend([acc_majority_voting, acc_soft_voting, acc_combined_filtered])\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(models, accuracies, color=['blue', 'orange', 'green', 'red', 'purple', 'brown', 'cyan', 'magenta', 'gray'])\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.xlabel('Models')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    # Predictions for ROC Analysis\n",
    "    filtered_predictions_binary = label_binarize(filtered_predictions, classes=[0, 1, 2, 3, 4, 5])\n",
    "    true_classes_binary = label_binarize(true_classes, classes=[0, 1, 2, 3, 4, 5])\n",
    "    \n",
    "    # ROC Analysis for Ensemble Model with Combined Filtered Predictions\n",
    "    fpr_combined_filtered = {}\n",
    "    tpr_combined_filtered = {}\n",
    "    roc_auc_combined_filtered = {}\n",
    "    for i in range(n_classes):\n",
    "        fpr_combined_filtered[i], tpr_combined_filtered[i], _ = roc_curve(true_classes_binary[:, i], filtered_predictions_binary[:, i])\n",
    "        roc_auc_combined_filtered[i] = auc(fpr_combined_filtered[i], tpr_combined_filtered[i])\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown']\n",
    "    for i, color in zip(range(n_classes), colors):\n",
    "        plt.plot(fpr_combined_filtered[i], tpr_combined_filtered[i], color=color, lw=2, label=f'ROC curve (class {i}) (area = {roc_auc_combined_filtered[i]:0.2f})')\n",
    "    \n",
    "    plt.plot([0, 1], [0, 1], 'k--', lw=2)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Analysis for Ensemble Model with Combined Filtered Predictions')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    return acc_combined_filtered,combined_filtered_model_path,model_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "n_classes = 6\n",
    "ft = 0\n",
    "# Set threshold for confidence score\n",
    "threshold = 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['DenseNet121', 'DenseNet169', 'InceptionResNetV2', 'ResNet152V2', 'DenseNet201']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code calculates class weights based on the distribution of classes in the training data. Let me explain each part:\n",
    "\n",
    "1. `from collections import Counter`: This imports the `Counter` class from the `collections` module. The `Counter` class is used for counting hashable objects.\n",
    "\n",
    "2. `counter = Counter(train_data.classes)`: This line creates a `Counter` object named `counter` by passing the `train_data.classes` as input. `train_data.classes` likely contains the class labels corresponding to the training data samples.\n",
    "\n",
    "3. `max_val = float(max(counter.values()))`: This line calculates the maximum count of any class in the training data by taking the maximum value from the counts stored in the `counter` object. It's converted to a float to ensure that division later on yields a float result.\n",
    "\n",
    "4. `class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}`: This line creates a dictionary named `class_weights`. It iterates over the items (class ID and count) in the `counter` object. For each class ID, it assigns a weight calculated as the maximum count (`max_val`) divided by the number of images in that class (`num_images`). This effectively gives more weight to underrepresented classes and less weight to overrepresented classes during training.\n",
    "\n",
    "5. `print(class_weights)`: Finally, this line prints out the calculated class weights.\n",
    "\n",
    "Overall, this code snippet is useful for addressing class imbalance issues in a classification task by assigning appropriate weights to different classes during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "counter = Counter(train_data.classes)                       \n",
    "max_val = float(max(counter.values()))   \n",
    "class_weights = {class_id : max_val/num_images for class_id, num_images in counter.items()}\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet is creating `ModelCheckpoint` instances for each model in the `models` list. Here's how it works:\n",
    "\n",
    "1. `checkpoints = []`: Initializes an empty list to store the `ModelCheckpoint` instances.\n",
    "\n",
    "2. Loop through each model name in the `models` list:\n",
    "\n",
    "   ```python\n",
    "   for model_name in models:\n",
    "   ```\n",
    "\n",
    "3. Define the filepath for saving the model weights:\n",
    "\n",
    "   ```python\n",
    "   filepath = f\"/kaggle/working/{model_name}.h5\"\n",
    "   ```\n",
    "\n",
    "   Here, the model weights will be saved in an HDF5 file format with the name of the model appended with \".h5\". The path specified is within the \"/kaggle/working\" directory.\n",
    "\n",
    "4. Create a `ModelCheckpoint` instance:\n",
    "\n",
    "   ```python\n",
    "   checkpoint = ModelCheckpoint(\n",
    "       filepath=filepath, \n",
    "       monitor='val_accuracy', \n",
    "       verbose=1,\n",
    "       save_best_only=True, \n",
    "       save_weights_only=False, \n",
    "       mode='auto'\n",
    "   )\n",
    "   ```\n",
    "\n",
    "   - `filepath`: Specifies the file path to save the model.\n",
    "   - `monitor`: Quantity to monitor (in this case, validation accuracy).\n",
    "   - `verbose`: Verbosity mode, where 1 indicates updating messages.\n",
    "   - `save_best_only`: Indicates whether to save only the best model.\n",
    "   - `save_weights_only`: Specifies whether to save the entire model (`False`) or just the model weights (`True`).\n",
    "   - `mode`: Defines the direction of improvement to monitor (`auto` in this case, which automatically decides the direction based on the monitored quantity).\n",
    "\n",
    "5. Append the created `ModelCheckpoint` instance to the `checkpoints` list:\n",
    "\n",
    "   ```python\n",
    "   checkpoints.append(checkpoint)\n",
    "   ```\n",
    "\n",
    "After this loop, the `checkpoints` list will contain `ModelCheckpoint` instances for each model, each specifying the filepath to save the best model weights based on validation accuracy. These checkpoints can be passed as callbacks during model training to save the best model weights automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = []\n",
    "\n",
    "# Loop through the models to create ModelCheckpoint instances with different filepaths\n",
    "for model_name in models:\n",
    "    filepath = f\"/kaggle/working/{model_name}.h5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=filepath, \n",
    "        monitor='val_accuracy', \n",
    "        verbose=1,\n",
    "        save_best_only=True, \n",
    "        save_weights_only=False, \n",
    "        mode='auto'\n",
    "    )\n",
    "    checkpoints.append(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code you provided initializes an `EarlyStopping` callback. Here's a breakdown of its parameters:\n",
    "\n",
    "- `monitor='val_accuracy'`: This parameter specifies the quantity to be monitored during training. In this case, it's validation accuracy.\n",
    "\n",
    "- `min_delta=0`: This parameter defines the minimum change in the monitored quantity to qualify as an improvement. If the change is less than this value, it won't be considered an improvement.\n",
    "\n",
    "- `patience=20`: This parameter indicates the number of epochs with no improvement after which training will be stopped. In this case, if there's no improvement in validation accuracy for 20 consecutive epochs, training will be stopped.\n",
    "\n",
    "- `verbose=1`: This parameter controls the verbosity of the output. A value of 1 means that progress messages will be printed.\n",
    "\n",
    "- `mode='auto'`: This parameter determines the direction of improvement to monitor. 'auto' mode automatically infers the direction based on the monitored quantity. Since the monitored quantity is validation accuracy, 'auto' will infer that an increase in validation accuracy is considered an improvement.\n",
    "\n",
    "- `restore_best_weights=True`: This parameter specifies whether to restore the model weights from the epoch with the best value of the monitored quantity. If set to `True`, the model weights will be restored to the best state found during training when training stops.\n",
    "\n",
    "Overall, this `EarlyStopping` callback will monitor the validation accuracy during training and stop training if there's no improvement for 20 consecutive epochs, while restoring the model weights to the best state found during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "early_stop = EarlyStopping(monitor='val_accuracy', min_delta=0, patience=20, verbose=1, mode='auto', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Evaluate and Ensemble models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_evaluate_ensemble(models, input_shape, n_classes, optimizer, ft)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
